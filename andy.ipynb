{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to 'combined.json'\n"
     ]
    }
   ],
   "source": [
    "# Load data from the first JSON file\n",
    "data_list1 = []\n",
    "with open('data/domain1_train.json', 'r') as f1:\n",
    "    for line in f1:\n",
    "        data_list1.append(json.loads(line))\n",
    "\n",
    "# Load data from the second JSON file\n",
    "data_list2 = []\n",
    "with open('data/domain2_train.json', 'r') as f2:\n",
    "    for line in f2:\n",
    "        data_list2.append(json.loads(line))\n",
    "\n",
    "# Combine the data from both lists\n",
    "combined_data = data_list1 + data_list2\n",
    "\n",
    "# Save the combined data to a new JSON file\n",
    "with open('data/combined.json', 'w') as output_file:\n",
    "    for item in combined_data:\n",
    "        json.dump(item, output_file)\n",
    "        output_file.write('\\n')\n",
    "\n",
    "print(\"Combined data saved to 'combined.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Combined Data to list and Train_Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON data\n",
    "data_list = []\n",
    "with open('data/combined.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Numeric sequences and corresponding labels\n",
    "sequences = df['text'].to_list()\n",
    "sequences = [\" \".join(map(str, seq)) for seq in sequences]\n",
    "labels =  df['label'].to_list() # Corresponding labels (1 for human, 0 for machine)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<34400x4990 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1346067 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function for tfidf for the convenience of changing: \n",
    "# input text (in training set, validation and test); max features needed; and ngrams\n",
    "def tfidf(cleanedText_train, cleanedText_test, maxFeatures = None, ngram = 1):\n",
    "    '''\n",
    "    This function take string form cleaned Text and process it to Tf-Idf sparse matrix\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(max_features = maxFeatures,ngram_range = (ngram,ngram))\n",
    "    trained_vec = vectorizer.fit(cleanedText_train)\n",
    "    tfidf = trained_vec.transform(cleanedText_test)\n",
    "    # check the names of terms\n",
    "    #display(vectorizer.get_feature_names_out())\n",
    "    return tfidf \n",
    "\n",
    "tfidf(sequences,sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert numeric sequences into BoW-like vectors\n",
    "bow_vectors = vectorizer.fit_transform([\" \".join(map(str, seq)) for seq in sequences])\n",
    "\n",
    "\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = LogisticRegression(solver='newton-cg', C=0.5, max_iter=10000)#MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
