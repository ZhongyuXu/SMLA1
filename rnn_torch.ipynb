{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[70, 746, 825, 109, 2083, 0, 2, 0, 0, 0, 9, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1209, 179, 1952, 4, 4959, 7, 0, 2, 978, 1522,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[287, 3, 3330, 0, 23, 12, 13, 465, 74, 8, 0, 8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 3, 592, 19, 2, 706, 1439, 2575, 7, 2, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9, 2, 110, 12, 42, 32, 44, 361, 9, 3860, 2358...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34395</th>\n",
       "      <td>[175, 1317, 38, 754, 9, 5, 0, 228, 1, 45, 6, 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34396</th>\n",
       "      <td>[466, 5, 70, 1242, 6, 3888, 1, 34, 43, 5, 70, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34397</th>\n",
       "      <td>[10, 0, 21, 1650, 18, 5, 1335, 1, 208, 5, 997,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34398</th>\n",
       "      <td>[18, 39, 316, 133, 365, 2019, 1, 27, 10, 5, 61...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34399</th>\n",
       "      <td>[10, 0, 859, 36, 860, 765, 250, 1, 872, 3, 27,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      [70, 746, 825, 109, 2083, 0, 2, 0, 0, 0, 9, 0,...      1\n",
       "1      [1209, 179, 1952, 4, 4959, 7, 0, 2, 978, 1522,...      1\n",
       "2      [287, 3, 3330, 0, 23, 12, 13, 465, 74, 8, 0, 8...      1\n",
       "3      [0, 0, 3, 592, 19, 2, 706, 1439, 2575, 7, 2, 0...      1\n",
       "4      [9, 2, 110, 12, 42, 32, 44, 361, 9, 3860, 2358...      1\n",
       "...                                                  ...    ...\n",
       "34395  [175, 1317, 38, 754, 9, 5, 0, 228, 1, 45, 6, 2...      0\n",
       "34396  [466, 5, 70, 1242, 6, 3888, 1, 34, 43, 5, 70, ...      0\n",
       "34397  [10, 0, 21, 1650, 18, 5, 1335, 1, 208, 5, 997,...      0\n",
       "34398  [18, 39, 316, 133, 365, 2019, 1, 27, 10, 5, 61...      0\n",
       "34399  [10, 0, 859, 36, 860, 765, 250, 1, 872, 3, 27,...      0\n",
       "\n",
       "[34400 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_1 = 'data/domain1_train.json'\n",
    "df1 = pd.read_json(file_path_1, lines=True)\n",
    "\n",
    "\n",
    "file_path_2 = 'data/domain2_train.json'\n",
    "df2 = pd.read_json(file_path_2, lines=True).drop('model', axis=1)\n",
    "\n",
    "\n",
    "df_comb = pd.concat([df1, df2],axis=0,ignore_index=True)\n",
    "\n",
    "df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_comb['text'].to_list()\n",
    "X = [\" \".join(map(str, x)) for x in X]\n",
    "# Define a tokenizer (you can choose the tokenizer that suits your data)\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "X = [tokenizer(text) for text in X]\n",
    "\n",
    "# Define the maximum sequence length you want to use\n",
    "max_seq_length = 50  # Adjust this based on your dataset and model requirements\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "X = [seq[:max_seq_length] + ['9999'] * (max_seq_length - len(seq)) if len(seq) < max_seq_length else seq[:max_seq_length] for seq in X]\n",
    "X = [[int(word) for word in sentence] for sentence in X]\n",
    "y =  df_comb['label'].to_list() # Corresponding labels (1 for human, 0 for machine)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "batch_size = 1  # Adjust this based on your dataset and model requirements\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 2\n",
    "num_epochs = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.005\n",
    "\n",
    "input_size = 50\n",
    "sequence_length = 28\n",
    "hidden_size = 50\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Andy\\OneDrive - The University of Melbourne\\SML\\A1\\rnn.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Andy\\OneDrive - The University of Melbourne\\SML\\A1\\rnn.ipynb Cell 9\u001b[0m line \u001b[0;36mRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m h0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# x: (n, 28, 28), h0: (2, n, 128)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Forward propagate RNN\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h0)  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# or:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#out, _ = self.lstm(x, (h0,c0))  \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Decode the hidden state of the last time step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X46sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m out \u001b[39m=\u001b[39m out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:483\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[39mif\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m         \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 483\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[39m{\u001b[39;00mhx\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    485\u001b[0m         hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    486\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) # input is X and h so have a size of input_size + hidden_size\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # changed input, output size is the number of classes\n",
    "        self.activation = nn.Tanh() # new\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.activation(self.i2h(combined)) # changed to use activation. This is using linear activation function above combined have a input size of input_size + hidden_size\n",
    "        output = self.h2o(hidden) # changed input\n",
    "        output = self.softmax(output) # we only use the output of the last word to make prediction despite we have output for every word. For extra long sentence, we loose the info at the front\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 80000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "noise_level = 0 # change this line (as discussed later)\n",
    "n_hidden = 50\n",
    "learning_rate = 0.005\n",
    "n_letters = max_seq_length\n",
    "n_categories = 2\n",
    "n_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7600, -0.6305]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1.])\n",
      "tensor([[-1.3956, -0.2846]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0.])\n",
      "tensor([[-1.0256, -0.4441]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0.])\n",
      "tensor([[-0.6926, -0.6937]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0.])\n",
      "tensor([[-0.7295, -0.6581]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "rnn = RNNClassifier(n_letters, n_hidden, n_categories)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# number of epoch is set by experience\n",
    "for epoch in range(n_epoch):  # Loop over training dataset `n_epochs` times\n",
    "\n",
    "    for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
    "        \n",
    "        x, labels = data\n",
    "\n",
    "        # print(x.shape)\n",
    "        # x = x.squeeze(1) # Flatten images but keep batch dimension\n",
    "\n",
    "        # fit one linear regression for each class and we pick the highest p~ as our prediction\n",
    "\n",
    "        for i in range(x.size()[0]):\n",
    "            new_shape = x[i].unsqueeze(0)\n",
    "            prediction, hidden = rnn(new_shape,  rnn.initHidden())\n",
    "        print(prediction)\n",
    "        print(labels)\n",
    "        break\n",
    "        # criterion is pre-set above as loss function\n",
    "        loss = criterion(prediction, target=labels)\n",
    "    \n",
    "        loss.backward()               # Backward pass (compute parameter gradients)\n",
    "        optimizer.step()              # Update weight parameter using SGD\n",
    "        optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "            # Add parameters' gradients to their values, multiplied by learning rate\n",
    "        for p in rnn.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (i2h): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (h2o): Linear(in_features=50, out_features=2, bias=True)\n",
       "  (activation): Tanh()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionalGRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionalGRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) \n",
    "        self.att = nn.Linear(hidden_size, 1) \n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        # process the input sequence into a sequence of RNN hidden states\n",
    "        states, _ = self.gru(input_sequence)\n",
    "        # compute attention scores to each RNN hidden state (we use a linear function)\n",
    "        att_scores = self.att(states)\n",
    "        # rescale the attention scores using a softmax, so they sum to one\n",
    "        alpha = F.softmax(att_scores, dim=0)\n",
    "        # compute the \"c\" vector as a weighted combination of the RNN hidden states\n",
    "        c = torch.sum(torch.mul(states, alpha), dim=0)\n",
    "        # now couple up the c state to the output, and compute log-softmax\n",
    "        output = self.h2o(c.view(1, -1)) \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionalGRUClassifier(n_letters, n_hidden, n_categories)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses_att = []\n",
    "current_loss = 0\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample(noise=noise_level)\n",
    "\n",
    "    model.zero_grad()\n",
    "    output, _ = model.forward(line_tensor)\n",
    "    output = torch.squeeze(output, 1) # remove redundant dimension\n",
    "    loss = criterion(output, category_tensor)\n",
    "    current_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = 'âœ“' if guess == category else 'âœ— (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses_att.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.rnn.bidirectional else hidden[-1, :, :])\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mTextRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbidirectional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ".. note::\n",
      "    As per the example above, an ``__init__()`` call to the parent class\n",
      "    must be made before assignment on the child.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "?TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "vocab_size = 5000\n",
    "embedding_dim = 128\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = TextRNN(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for feature, label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(feature).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,   13,   22,  ..., 9999, 9999, 9999],\n",
      "        [4319, 3468,    2,  ..., 9999, 9999, 9999],\n",
      "        [2344,   93,    3,  ...,   15,    1,    0],\n",
      "        ...,\n",
      "        [   2,  107,    1,  ...,  350,   32,   23],\n",
      "        [   5,  921, 3048,  ...,    2,    0, 1402],\n",
      "        [  10, 1682,   61,  ..., 9999, 9999, 9999]])\n",
      "tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for feature, label in train_loader:\n",
    "    print(feature)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Andy\\OneDrive - The University of Melbourne\\SML\\A1\\rnn.ipynb Cell 16\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m N_EPOCHS \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(model, train_loader, optimizer, criterion)\n",
      "\u001b[1;32mc:\\Users\\Andy\\OneDrive - The University of Melbourne\\SML\\A1\\rnn.ipynb Cell 16\u001b[0m line \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m feature, label \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     predictions \u001b[39m=\u001b[39m model(feature)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(predictions, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Andy\\OneDrive - The University of Melbourne\\SML\\A1\\rnn.ipynb Cell 16\u001b[0m line \u001b[0;36mTextRNN.forward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     output, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embedded)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/OneDrive%20-%20The%20University%20of%20Melbourne/SML/A1/rnn.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, :, :], hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, :]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mbidirectional \u001b[39melse\u001b[39;00m hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, :])\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\Andy\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "N_EPOCHS = 5\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze(1)\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    accuracy = torch.sum(rounded_predictions == y_test).item() / len(y_test)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
